#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–æ—Ä–Ω–æ–≥–æ –¥–µ–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama
–î–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞—Ç–∞–ª–æ–≥–µ app/
"""

import json
import requests
import numpy as np
import re
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import os
import sys
from collections import Counter
from abc import ABC, abstractmethod

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    from sentence_transformers import SentenceTransformer
    SEMANTIC_AVAILABLE = True
except ImportError:
    SEMANTIC_AVAILABLE = False
    print("‚ö†Ô∏è  sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class VectorStoreInterface(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
    
    @abstractmethod
    def add_documents(self, documents: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        pass
    
    @abstractmethod
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        pass

@dataclass
class OllamaConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Ollama"""
    host: str
    headers: Dict[str, str]
    model: str = "yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest"
    timeout: int = 30

class OllamaLLM:
    """LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama"""
    
    def __init__(self, config: OllamaConfig):
        self.config = config
        self.base_url = f"https://{config.host}"
    
    def generate(self, prompt: str, context: str = "", max_tokens: int = 500) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Ollama"""
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if context:
            full_prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ –≥–æ—Ä–Ω–æ–º—É –¥–µ–ª—É:
{context}

–í–æ–ø—Ä–æ—Å: {prompt}

–û—Ç–≤–µ—Ç (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ):"""
        else:
            full_prompt = f"–í–æ–ø—Ä–æ—Å: {prompt}\n\n–û—Ç–≤–µ—Ç (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ):"
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                headers=self.config.headers,
                json={
                    "model": self.config.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": max_tokens
                    }
                },
                timeout=self.config.timeout,
                verify=False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞")
            else:
                return f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}"
                
        except requests.exceptions.Timeout:
            return "–û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama"
        except requests.exceptions.ConnectionError:
            return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}"
    
    def test_connection(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                headers=self.config.headers,
                timeout=5,
                verify=False
            )
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                headers=self.config.headers,
                timeout=5,
                verify=False
            )
            if response.status_code == 200:
                return response.json().get('models', [])
            return []
        except:
            return []

class SimpleVectorStore(VectorStoreInterface):
    """–ü—Ä–æ—Å—Ç–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF"""
    
    def __init__(self):
        self.documents = []
        self.vocabulary = set()
        self.tf_idf_matrix = []
        self.idf_scores = {}
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        self.documents = documents
        self._build_tfidf_index()
    
    def _build_tfidf_index(self):
        """–°—Ç—Ä–æ–∏—Ç TF-IDF –∏–Ω–¥–µ–∫—Å"""
        if not self.documents:
            return
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        all_texts = []
        for doc in self.documents:
            text = f"{doc['question']} {doc['answer']}"
            words = self._preprocess_text(text)
            all_texts.append(words)
            self.vocabulary.update(words)
        
        self.vocabulary = list(self.vocabulary)
        
        if not self.vocabulary:
            return
        
        # –í—ã—á–∏—Å–ª—è–µ–º IDF
        total_docs = len(all_texts)
        for word in self.vocabulary:
            doc_count = sum(1 for text in all_texts if word in text)
            self.idf_scores[word] = np.log(total_docs / doc_count) if doc_count > 0 else 0
        
        # –í—ã—á–∏—Å–ª—è–µ–º TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for text in all_texts:
            tf_scores = Counter(text)
            tf_idf_vector = []
            
            for word in self.vocabulary:
                tf = tf_scores.get(word, 0) / len(text) if len(text) > 0 else 0
                idf = self.idf_scores[word]
                tf_idf_vector.append(tf * idf)
            
            self.tf_idf_matrix.append(tf_idf_vector)
    
    def _preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        return [word for word in words if len(word) >= 3]
    
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        if not self.documents or not self.tf_idf_matrix:
            return []
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
        query_words = self._preprocess_text(query)
        if not query_words:
            return []
        
        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_tf = Counter(query_words)
        query_vector = []
        
        for word in self.vocabulary:
            tf = query_tf.get(word, 0) / len(query_words)
            idf = self.idf_scores.get(word, 0)
            query_vector.append(tf * idf)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for i, doc_vector in enumerate(self.tf_idf_matrix):
            similarity = self._cosine_similarity(query_vector, doc_vector)
            similarities.append((i, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for i, (doc_idx, similarity) in enumerate(similarities[:top_k]):
            if similarity > 0:
                doc = self.documents[doc_idx]
                results.append({
                    'question': doc['question'],
                    'answer': doc['answer'],
                    'similarity': similarity,
                    'rank': i + 1
                })
        
        return results
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        if len(vec1) != len(vec2):
            return 0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = np.sqrt(sum(a * a for a in vec1))
        magnitude2 = np.sqrt(sum(a * a for a in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0
        
        return dot_product / (magnitude1 * magnitude2)

class SemanticVectorStore(VectorStoreInterface):
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        if not SEMANTIC_AVAILABLE:
            raise ImportError("sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
        
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        self.documents = documents
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts = []
        for doc in documents:
            text = f"{doc['question']} {doc['answer']}"
            texts.append(text)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.embeddings = self.model.encode(texts, show_progress_bar=True)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        if not self.documents or self.embeddings is None:
            return []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            similarity = self._cosine_similarity(query_embedding[0], doc_embedding)
            similarities.append((i, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for i, (doc_idx, similarity) in enumerate(similarities[:top_k]):
            if similarity > 0:
                doc = self.documents[doc_idx]
                results.append({
                    'question': doc['question'],
                    'answer': doc['answer'],
                    'similarity': float(similarity),
                    'rank': i + 1
                })
        
        return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        dot_product = np.dot(vec1, vec2)
        magnitude1 = np.linalg.norm(vec1)
        magnitude2 = np.linalg.norm(vec2)
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)

class HybridVectorStore(VectorStoreInterface):
    """–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (TF-IDF + —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫)"""
    
    def __init__(self, semantic_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.tfidf_store = SimpleVectorStore()
        self.semantic_store = None
        
        if SEMANTIC_AVAILABLE:
            self.semantic_store = SemanticVectorStore(semantic_model)
        else:
            print("‚ö†Ô∏è  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ TF-IDF")
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ–±–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ TF-IDF
        self.tfidf_store.add_documents(documents)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if self.semantic_store:
            self.semantic_store.add_documents(documents)
    
    def search(self, query: str, top_k: int = 3, tfidf_weight: float = 0.3, semantic_weight: float = 0.7) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≤–µ—Å–∞–º–∏ –¥–ª—è TF-IDF –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        results = []
        
        # TF-IDF –ø–æ–∏—Å–∫
        tfidf_results = self.tfidf_store.search(query, top_k * 2)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.semantic_store:
            semantic_results = self.semantic_store.search(query, top_k * 2)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
            combined_scores = {}
            
            for result in tfidf_results:
                key = f"{result['question']}|{result['answer']}"
                combined_scores[key] = {
                    'question': result['question'],
                    'answer': result['answer'],
                    'tfidf_score': result['similarity'],
                    'semantic_score': 0.0,
                    'combined_score': result['similarity'] * tfidf_weight
                }
            
            for result in semantic_results:
                key = f"{result['question']}|{result['answer']}"
                if key in combined_scores:
                    combined_scores[key]['semantic_score'] = result['similarity']
                    combined_scores[key]['combined_score'] += result['similarity'] * semantic_weight
                else:
                    combined_scores[key] = {
                        'question': result['question'],
                        'answer': result['answer'],
                        'tfidf_score': 0.0,
                        'semantic_score': result['similarity'],
                        'combined_score': result['similarity'] * semantic_weight
                    }
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
            sorted_results = sorted(combined_scores.values(), key=lambda x: x['combined_score'], reverse=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for i, result in enumerate(sorted_results[:top_k]):
                if result['combined_score'] > 0:
                    results.append({
                        'question': result['question'],
                        'answer': result['answer'],
                        'similarity': result['combined_score'],
                        'tfidf_score': result['tfidf_score'],
                        'semantic_score': result['semantic_score'],
                        'rank': i + 1
                    })
        else:
            # –¢–æ–ª—å–∫–æ TF-IDF, –µ—Å–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            results = tfidf_results[:top_k]
        
        return results

class MiningRAG:
    """RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–æ—Ä–Ω–æ–≥–æ –¥–µ–ª–∞"""
    
    def __init__(self, ollama_config: OllamaConfig, search_type: str = "tfidf"):
        self.llm = OllamaLLM(ollama_config)
        self.search_type = search_type
        self.qa_pairs = []
        self.knowledge_base_path = None
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if search_type == "tfidf":
            self.vector_store = SimpleVectorStore()
        elif search_type == "semantic":
            if not SEMANTIC_AVAILABLE:
                print("‚ö†Ô∏è  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è TF-IDF")
                self.vector_store = SimpleVectorStore()
                self.search_type = "tfidf"
            else:
                self.vector_store = SemanticVectorStore()
        elif search_type == "hybrid":
            self.vector_store = HybridVectorStore()
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞: {search_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: tfidf, semantic, hybrid")
    
    def load_knowledge_base(self, json_file_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.qa_pairs = data['qa_pairs']
            self.knowledge_base_path = json_file_path
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.qa_pairs)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
            
            # –°—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            self.vector_store.add_documents(self.qa_pairs)
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω")
            
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_file_path}")
            return False
        except json.JSONDecodeError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
        
        return True
    
    def ask_question(self, question: str, use_rag: bool = True) -> Dict[str, Any]:
        """–ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–µ"""
        if not question.strip():
            return {
                "answer": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.",
                "sources": [],
                "confidence": 0.0,
                "method": "error"
            }
        
        if use_rag and self.qa_pairs:
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            search_results = self.vector_store.search(question, top_k=3)
            
            if search_results:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context = "\n\n".join([
                    f"–í–æ–ø—Ä–æ—Å: {result['question']}\n–û—Ç–≤–µ—Ç: {result['answer']}"
                    for result in search_results
                ])
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM
                answer = self.llm.generate(question, context)
                
                return {
                    "answer": answer,
                    "sources": search_results,
                    "confidence": max(result['similarity'] for result in search_results),
                    "method": f"rag_{self.search_type}"
                }
            else:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                answer = self.llm.generate(question)
                return {
                    "answer": answer,
                    "sources": [],
                    "confidence": 0.0,
                    "method": "llm_only"
                }
        else:
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –±–µ–∑ LLM
            search_results = self.vector_store.search(question, top_k=1)
            if search_results:
                return {
                    "answer": search_results[0]['answer'],
                    "sources": search_results,
                    "confidence": search_results[0]['similarity'],
                    "method": "direct_search"
                }
            else:
                return {
                    "answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                    "sources": [],
                    "confidence": 0.0,
                    "method": "not_found"
                }
    
    def test_system(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º—É"""
        print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã...")
        
        # –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama
        if self.llm.test_connection():
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama —É—Å–ø–µ—à–Ω–æ")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
            return False
        
        # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        if self.qa_pairs:
            print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.qa_pairs)} –ø–∞—Ä")
        else:
            print("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return False
        
        # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
        test_question = "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–ø–æ—Å—Ç–∏?"
        search_results = self.vector_store.search(test_question, top_k=1)
        if search_results:
            print(f"‚úÖ –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç: –Ω–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞")
            return False
        
        print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        return True
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "knowledge_base_loaded": len(self.qa_pairs) > 0,
            "total_qa_pairs": len(self.qa_pairs),
            "ollama_connected": self.llm.test_connection(),
            "knowledge_base_path": self.knowledge_base_path,
            "vector_index_built": len(self.vector_store.tf_idf_matrix) > 0
        }

def create_ollama_config(host: str, token: str, model: str = "yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest") -> OllamaConfig:
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Ollama"""
    return OllamaConfig(
        host=host,
        headers={'X-Access-Token': token},
        model=model
    )

def demo_rag_system(knowledge_base_path: str = None, search_type: str = "tfidf"):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("üè≠ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø RAG –°–ò–°–¢–ï–ú–´ –î–õ–Ø –ì–û–†–ù–û–ì–û –î–ï–õ–ê")
    print("=" * 60)
    print(f"üîç –¢–∏–ø –ø–æ–∏—Å–∫–∞: {search_type.upper()}")
    print("=" * 60)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Ollama
    ollama_config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    # –°–æ–∑–¥–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
    rag = MiningRAG(ollama_config, search_type=search_type)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    models = rag.llm.get_available_models()
    if models:
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for model in models:
            print(f"  - {model['name']}")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    if knowledge_base_path is None:
        knowledge_base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 
                                         'output', 'base-mining-and-mining-quality.final.json')
    
    print(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑: {knowledge_base_path}")
    
    if not rag.load_knowledge_base(knowledge_base_path):
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        return
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    if not rag.test_system():
        print("‚ùå –°–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
        return
    
    # –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
    test_questions = [
        "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–ø–æ—Å—Ç–∏ –ü—Ä–æ—Ç–æ–¥—å—è–∫–æ–Ω–æ–≤–∞?",
        "–ö–∞–∫–∞—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ–ª—â–∏–Ω–∞ –≤—ã—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –≤–∑—Ä—ã–≤–Ω—ã—Ö —Ä–∞–±–æ—Ç?",
        "–ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª–∏–Ω–∏—é –ø–∞–¥–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∏–º–∞–µ–º–æ—Å—Ç–∏ –ø–æ—Ä–æ–¥—ã?",
        "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —Å–ø–æ—Å–æ–± –∫—Ä–µ–ø–ª–µ–Ω–∏—è –¥–ª—è –≥–æ—Ä–Ω—ã—Ö –≤—ã—Ä–∞–±–æ—Ç–æ–∫?"
    ]
    
    print("\n‚ùì –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–û–ü–†–û–°–û–í:")
    print("-" * 40)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. –í–æ–ø—Ä–æ—Å: {question}")
        print("ü§î –î—É–º–∞—é...")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        result = rag.ask_question(question)
        
        print(f"üí° –û—Ç–≤–µ—Ç: {result['answer']}")
        print(f"üìä –ú–µ—Ç–æ–¥: {result['method']}")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
        
        if result['sources']:
            print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {len(result['sources'])}")
            for j, source in enumerate(result['sources'][:2], 1):
                print(f"  {j}. {source['question'][:60]}... (—Å—Ö–æ–¥—Å—Ç–≤–æ: {source['similarity']:.2f})")
        
        print("-" * 60)
    
    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

def interactive_mode(knowledge_base_path: str = None, search_type: str = "tfidf"):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –∑–∞–¥–∞–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
    print("üí¨ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú")
    print(f"üîç –¢–∏–ø –ø–æ–∏—Å–∫–∞: {search_type.upper()}")
    print("–í–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    print("=" * 40)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Ollama
    ollama_config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    # –°–æ–∑–¥–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
    rag = MiningRAG(ollama_config, search_type=search_type)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    if knowledge_base_path is None:
        knowledge_base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 
                                         'output', 'base-mining-and-mining-quality.final.json')
    
    print(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑: {knowledge_base_path}")
    
    if not rag.load_knowledge_base(knowledge_base_path):
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        return
    
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    while True:
        try:
            question = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            
            if question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not question:
                continue
            
            print("ü§î –î—É–º–∞—é...")
            result = rag.ask_question(question)
            
            print(f"\nüí° –û—Ç–≤–µ—Ç: {result['answer']}")
            print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
            print(f"üîç –ú–µ—Ç–æ–¥: {result['method']}")
            
            if result['sources']:
                print(f"üìö –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(result['sources'])}")
        
        except KeyboardInterrupt:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–æ—Ä–Ω–æ–≥–æ –¥–µ–ª–∞')
    parser.add_argument('--mode', choices=['demo', 'interactive'], default='demo',
                       help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: demo –∏–ª–∏ interactive')
    parser.add_argument('--model', default='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest',
                       help='–ú–æ–¥–µ–ª—å Ollama –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--data', '--knowledge-base', 
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (JSON)')
    parser.add_argument('--list-data', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
    parser.add_argument('--search-type', choices=['tfidf', 'semantic', 'hybrid'], 
                       default='tfidf', help='–¢–∏–ø –ø–æ–∏—Å–∫–∞: tfidf, semantic, hybrid')
    
    args = parser.parse_args()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if args.list_data:
        print("üìö –î–û–°–¢–£–ü–ù–´–ï –§–ê–ô–õ–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:")
        print("=" * 40)
        
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output')
        if os.path.exists(output_dir):
            json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
            for i, file in enumerate(json_files, 1):
                file_path = os.path.join(output_dir, file)
                file_size = os.path.getsize(file_path)
                print(f"{i}. {file} ({file_size:,} –±–∞–π—Ç)")
        else:
            print("‚ùå –ö–∞—Ç–∞–ª–æ–≥ output –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(0)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    knowledge_base_path = args.data
    if knowledge_base_path and not os.path.isabs(knowledge_base_path):
        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ output
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output')
        knowledge_base_path = os.path.join(output_dir, knowledge_base_path)
    
    if args.mode == 'demo':
        demo_rag_system(knowledge_base_path, args.search_type)
    elif args.mode == 'interactive':
        interactive_mode(knowledge_base_path, args.search_type)
