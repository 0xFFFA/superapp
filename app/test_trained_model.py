#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
from pathlib import Path

def load_trained_model(base_model_path, lora_adapters_path=None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ –∏–ª–∏ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
    
    Args:
        base_model_path: –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        lora_adapters_path: –ü—É—Ç—å –∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        model, tokenizer: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print(f"–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–∑: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    
    if lora_adapters_path and Path(lora_adapters_path).exists():
        print(f"–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏–∑: {lora_adapters_path}")
        model = PeftModel.from_pretrained(base_model, lora_adapters_path)
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    else:
        print("‚ö†Ô∏è LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
        model = base_model
        print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    return model, tokenizer

def test_model(model, tokenizer, questions, max_length=200, temperature=0.7):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–ø–∏—Å–∫–µ –≤–æ–ø—Ä–æ—Å–æ–≤
    
    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        questions: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    print("\n" + "="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
    device = next(model.parameters()).device
    print(f"–ú–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
    
    for i, question in enumerate(questions, 1):
        print(f"\n{i}. –í–æ–ø—Ä–æ—Å: {question}")
        print("-" * 40)
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"### –í–æ–ø—Ä–æ—Å: {question}\n### –û—Ç–≤–µ—Ç:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = tokenizer(prompt, return_tensors="pt")
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏ –≤—Å–µ –ø–æ—Å–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ ###)
            if "### –û—Ç–≤–µ—Ç:" in answer:
                answer = answer.split("### –û—Ç–≤–µ—Ç:")[1].strip()
                # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Å–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ ### –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                if "###" in answer:
                    answer = answer.split("###")[0].strip()
            
            print(f"–û—Ç–≤–µ—Ç: {answer}")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            import traceback
            traceback.print_exc()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º
    base_model_path = "models/qwen-2.5-1.5b"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 1.5B –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞
    lora_adapters_path = "trained_model/v1"  # –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–µ–π
    if not Path(base_model_path).exists():
        print(f"‚ùå –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_model_path}")
        return
    
    if not Path(lora_adapters_path).exists():
        print(f"‚ùå LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {lora_adapters_path}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        return
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        model, tokenizer = load_trained_model(base_model_path, lora_adapters_path)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_questions = [
            # –í–æ–ø—Ä–æ—Å—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            "–ß—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —à–∞—Ä–æ–≤–∞—è –º–µ–ª—å–Ω–∏—Ü–∞?",
            "–ö–∞–∫–æ–π —Ç–∏–ø –¥–∏—Å–ø–µ—Ä–≥–∞—Ç–æ—Ä–æ–≤ –ø–æ–ª—É—á–∏–ª –Ω–∞–∏–±–æ–ª—å—à–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ?",
            "–ö–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–º–µ–µ—Ç –¥–∏–∞–º–µ—Ç—Ä —à–∞—Ä–æ–≤ –ø—Ä–∏ –¥–∏—Å–ø–µ—Ä–≥–∏—Ä–æ–≤–∞–Ω–∏–∏?",
            "–ö–∞–∫ –º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É –≤—Ä–∞—â–µ–Ω–∏—è –±–∞—Ä–∞–±–∞–Ω–∞ –¥–ª—è –ª–∞–≤–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–≤–∏–∂–µ–Ω–∏—è —à–∞—Ä–æ–≤?",
            
            # –ù–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–æ–±—â–µ–Ω–∏—è
            "–ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–º–µ–µ—Ç —à–∞—Ä–æ–≤–∞—è –º–µ–ª—å–Ω–∏—Ü–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –¥–∏—Å–ø–µ—Ä–≥–∞—Ç–æ—Ä–∞–º–∏?",
            "–ö–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞—Ä–æ–≤–æ–π –º–µ–ª—å–Ω–∏—Ü—ã?",
            "–í –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞—Ä–æ–≤—É—é –º–µ–ª—å–Ω–∏—Ü—É?",
            "–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —à–∞—Ä–æ–≤–æ–π –º–µ–ª—å–Ω–∏—Ü—ã?"
        ]
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        test_model(model, tokenizer, test_questions)
        
        print("\n" + "="*60)
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("="*60)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
