#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã
"""

import os
import sys
import json
from rag_system import MiningRAG, create_ollama_config

def test_ollama_connection():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama...")
    
    config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    rag = MiningRAG(config)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    if rag.llm.test_connection():
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama —É—Å–ø–µ—à–Ω–æ")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models = rag.llm.get_available_models()
        if models:
            print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
            for model in models:
                print(f"  - {model['name']} (—Ä–∞–∑–º–µ—Ä: {model.get('size', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')})")
        else:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
        
        return True
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
        return False

def test_knowledge_base_loading(knowledge_base_path: str = None):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    print("\nüìö –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    
    config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    rag = MiningRAG(config)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    if knowledge_base_path is None:
        knowledge_base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 
                                         'output', 'base-mining-and-mining-quality.final.json')
    
    print(f"üìÅ –ü—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {knowledge_base_path}")
    
    if not os.path.exists(knowledge_base_path):
        print(f"‚ùå –§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {knowledge_base_path}")
        return False
    
    if rag.load_knowledge_base(knowledge_base_path):
        print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = rag.get_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  - –ü–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç: {stats['total_qa_pairs']}")
        print(f"  - –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {'‚úÖ' if stats['vector_index_built'] else '‚ùå'}")
        
        return True
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        return False

def test_search_functionality():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞"""
    print("\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞...")
    
    config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    rag = MiningRAG(config)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    knowledge_base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 
                                     'output', 'base-mining-and-mining-quality.final.json')
    
    if not rag.load_knowledge_base(knowledge_base_path):
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        return False
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–ø–æ—Å—Ç–∏",
        "–≤–∑—Ä—ã–≤–Ω—ã–µ —Ä–∞–±–æ—Ç—ã",
        "–≥–æ—Ä–Ω—ã–µ –≤—ã—Ä–∞–±–æ—Ç–∫–∏",
        "–ø–æ—Ä–æ–¥–∞ –∏–∑–≤–µ—Å—Ç–Ω—è–∫"
    ]
    
    for query in test_queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        results = rag.vector_store.search(query, top_k=2)
        
        if results:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result['question'][:50]}... (—Å—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.3f})")
        else:
            print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return True

def test_llm_generation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤ LLM"""
    print("\nü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤...")
    
    config = create_ollama_config(
        host='193.247.73.14:11436',
        token='k6Svw7EldnQLhBpivenz7E2Z01H8FF',
        model='yandex/YandexGPT-5-Lite-8B-instruct-GGUF:latest'
    )
    
    rag = MiningRAG(config)
    
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    print("üîç –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
    simple_answer = rag.llm.generate("–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–æ—Ä–Ω–æ–µ –¥–µ–ª–æ?")
    print(f"–û—Ç–≤–µ—Ç: {simple_answer[:100]}...")
    
    # –¢–µ—Å—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    print("\nüîç –¢–µ—Å—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
    context = "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–ø–æ—Å—Ç–∏ –ü—Ä–æ—Ç–æ–¥—å—è–∫–æ–Ω–æ–≤–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ f = œÉ—Å–∂/100. –î–ª—è —Å–ª–∞–Ω—Ü–µ–≤ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç f = 2.5 - 3.0."
    contextual_answer = rag.llm.generate("–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫—Ä–µ–ø–æ—Å—Ç–∏?", context)
    print(f"–û—Ç–≤–µ—Ç: {contextual_answer[:100]}...")
    
    return True

def run_full_test(knowledge_base_path: str = None):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã"""
    print("üß™ –ü–û–õ–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 50)
    
    tests = [
        ("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama", test_ollama_connection),
        ("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π", lambda: test_knowledge_base_loading(knowledge_base_path)),
        ("–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞", test_search_functionality),
        ("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM", test_llm_generation)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ '{test_name}': {e}")
            results.append((test_name, False))
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\n{'='*50}")
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 50)
    
    passed = 0
    for test_name, success in results:
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if success else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print(f"{test_name}: {status}")
        if success:
            passed += 1
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{len(results)} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")
    
    if passed == len(results):
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
    else:
        print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–≤–∞–ª–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
    
    return passed == len(results)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã')
    parser.add_argument('--test', choices=['connection', 'knowledge', 'search', 'llm', 'all'], 
                       default='all', help='–ö–∞–∫–æ–π —Ç–µ—Å—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å')
    parser.add_argument('--data', '--knowledge-base', 
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (JSON)')
    parser.add_argument('--list-data', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
    
    args = parser.parse_args()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if args.list_data:
        print("üìö –î–û–°–¢–£–ü–ù–´–ï –§–ê–ô–õ–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:")
        print("=" * 40)
        
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output')
        if os.path.exists(output_dir):
            json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
            for i, file in enumerate(json_files, 1):
                file_path = os.path.join(output_dir, file)
                file_size = os.path.getsize(file_path)
                print(f"{i}. {file} ({file_size:,} –±–∞–π—Ç)")
        else:
            print("‚ùå –ö–∞—Ç–∞–ª–æ–≥ output –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    knowledge_base_path = args.data
    if knowledge_base_path and not os.path.isabs(knowledge_base_path):
        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ output
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output')
        knowledge_base_path = os.path.join(output_dir, knowledge_base_path)
    
    if args.test == 'connection':
        test_ollama_connection()
    elif args.test == 'knowledge':
        test_knowledge_base_loading(knowledge_base_path)
    elif args.test == 'search':
        test_search_functionality()
    elif args.test == 'llm':
        test_llm_generation()
    elif args.test == 'all':
        run_full_test(knowledge_base_path)
