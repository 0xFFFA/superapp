#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–µ–π
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –≤ –æ—Ç–≤–µ—Ç–∞—Ö –¥–æ –∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_models(base_model_path, lora_adapters_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∏ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª–∏"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª–∏...")
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    
    # –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA
    print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {lora_adapters_path}")
    trained_model = PeftModel.from_pretrained(base_model, lora_adapters_path)
    
    print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
    return base_model, trained_model, tokenizer

def generate_answer(model, tokenizer, question, max_length=200, temperature=0.7):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
    prompt = f"### –í–æ–ø—Ä–æ—Å: {question}\n### –û—Ç–≤–µ—Ç:"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
    if "### –û—Ç–≤–µ—Ç:" in answer:
        answer = answer.split("### –û—Ç–≤–µ—Ç:")[1].strip()
        if "###" in answer:
            answer = answer.split("###")[0].strip()
    
    return answer

def compare_models(base_model, trained_model, tokenizer, questions):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–∞–∑–æ–≤–æ–π –∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*80)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*80)
    
    for i, question in enumerate(questions, 1):
        print(f"\n{i}. –í–æ–ø—Ä–æ—Å: {question}")
        print("-" * 60)
        
        try:
            # –û—Ç–≤–µ—Ç –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            print("üîµ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:")
            base_answer = generate_answer(base_model, tokenizer, question)
            print(f"   {base_answer}")
            
            print("\nüü¢ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å:")
            trained_answer = generate_answer(trained_model, tokenizer, question)
            print(f"   {trained_answer}")
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
            print("\nüìä –ê–Ω–∞–ª–∏–∑:")
            if base_answer == trained_answer:
                print("   ‚ö†Ô∏è  –û—Ç–≤–µ—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å)")
            else:
                print("   ‚úÖ –û—Ç–≤–µ—Ç—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è (–º–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∞—Å—å)")
                if len(trained_answer) > len(base_answer):
                    print("   üìà –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–µ—Ç –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã")
                elif len(trained_answer) < len(base_answer):
                    print("   üìâ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–µ—Ç –±–æ–ª–µ–µ –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        
        print("\n" + "="*60)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–µ–π"
    )
    parser.add_argument(
        "--base-model", 
        default="/home/dev/industrial-ai-trainer/models/qwen-2.5-3b",
        help="–ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: qwen-2.5-3b)"
    )
    parser.add_argument(
        "--trained-model", 
        default="./trained_model/v1",
        help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ./trained_model/v1)"
    )
    parser.add_argument(
        "--questions", 
        nargs="+",
        default=[
            "–ß—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —à–∞—Ä–æ–≤–∞—è –º–µ–ª—å–Ω–∏—Ü–∞?",
            "–ö–∞–∫–æ–π —Ç–∏–ø –¥–∏—Å–ø–µ—Ä–≥–∞—Ç–æ—Ä–æ–≤ –ø–æ–ª—É—á–∏–ª –Ω–∞–∏–±–æ–ª—å—à–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ?",
            "–ö–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–º–µ–µ—Ç –¥–∏–∞–º–µ—Ç—Ä —à–∞—Ä–æ–≤ –ø—Ä–∏ –¥–∏—Å–ø–µ—Ä–≥–∏—Ä–æ–≤–∞–Ω–∏–∏?",
            "–ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–º–µ–µ—Ç —à–∞—Ä–æ–≤–∞—è –º–µ–ª—å–Ω–∏—Ü–∞?",
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —à–∞—Ä–æ–≤–∞—è –º–µ–ª—å–Ω–∏—Ü–∞?"
        ],
        help="–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã)"
    )
    parser.add_argument(
        "--max-length", 
        type=int, 
        default=200,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 200)"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.7,
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.7)"
    )
    
    args = parser.parse_args()
    
    print("üè≠ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–µ–π")
    print(f"üîµ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {args.base_model}")
    print(f"üü¢ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {args.trained_model}")
    print(f"‚ùì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(args.questions)}")
    print(f"üå°Ô∏è  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {args.temperature}")
    print(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {args.max_length}")
    print("-" * 80)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        base_model, trained_model, tokenizer = load_models(args.base_model, args.trained_model)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
        compare_models(base_model, trained_model, tokenizer, args.questions)
        
        print("\n" + "="*80)
        print("–°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("="*80)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
