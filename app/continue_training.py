#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
"""

import json
import os
import argparse
from pathlib import Path
from typing import Dict, List, Optional
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
    PeftModel
)
from datasets import Dataset
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ContinueTrainingTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, 
                 base_model: str = "meta-llama/Llama-2-7b-hf",
                 trained_model_path: str = "trained_model",
                 output_dir: str = "trained_model_continued",
                 device: str = "auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)
            trained_model_path: –ü—É—Ç—å –∫ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (auto/cpu/cuda)
        """
        self.base_model = base_model
        self.trained_model_path = Path(trained_model_path)
        self.output_dir = Path(output_dir)
        self.device = device
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è")
        logger.info(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
        logger.info(f"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {trained_model_path}")
        logger.info(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    
    def load_trained_model_and_tokenizer(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        logger.info("–ó–∞–≥—Ä—É–∂–∞—é —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            if (self.trained_model_path / "tokenizer.json").exists():
                logger.info("–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.trained_model_path,
                    trust_remote_code=True,
                    padding_side="right"
                )
            else:
                # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                logger.info("–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model,
                    trust_remote_code=True,
                    padding_side="right"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            logger.info("–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map=self.device,
                trust_remote_code=True
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è
            base_model = prepare_model_for_kbit_training(base_model)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            logger.info("–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
            if (self.trained_model_path / "adapter_config.json").exists():
                self.model = PeftModel.from_pretrained(base_model, self.trained_model_path)
                logger.info("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            else:
                logger.warning("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–µ...")
                self.setup_lora_config()
                self.model = get_peft_model(base_model, self.lora_config)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é
            self.model.print_trainable_parameters()
            
            logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def setup_lora_config(self, 
                          r: int = 16,
                          lora_alpha: int = 32,
                          lora_dropout: float = 0.1,
                          target_modules: Optional[List[str]] = None):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        
        Args:
            r: –†–∞–Ω–≥ LoRA
            lora_alpha: –ê–ª—å—Ñ–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä LoRA
            lora_dropout: Dropout –¥–ª—è LoRA
            target_modules: –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è LoRA
        """
        logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        if target_modules is None:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            bias="none"
        )
        
        logger.info("LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")
    
    def load_training_data(self, data_path: str) -> Dataset:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            data_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑: {data_path}")
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
            if 'qa_pairs' in data:
                qa_pairs = data['qa_pairs']
            elif isinstance(data, list):
                qa_pairs = data
            else:
                raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_texts = []
            for qa in qa_pairs:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                text = f"### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.\n\n### –í–æ–ø—Ä–æ—Å: {qa['question']}\n\n### –û—Ç–≤–µ—Ç: {qa['answer']}\n\n### –ö–æ–Ω–µ—Ü"
                training_texts.append(text)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –°–æ–∑–¥–∞–µ–º Dataset
            dataset = Dataset.from_dict({"text": training_texts})
            return dataset
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def tokenize_function(self, examples):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
    
    def setup_training(self, 
                      train_dataset: Dataset,
                      learning_rate: float = 1e-4,  # –°–Ω–∏–∂–∞–µ–º LR –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
                      num_epochs: int = 2,          # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
                      batch_size: int = 4,
                      gradient_accumulation_steps: int = 4):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            train_dataset: –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—Å–Ω–∏–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–º–µ–Ω—å—à–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            gradient_accumulation_steps: –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è...")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        tokenized_dataset = train_dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            fp16=True,
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            save_strategy="steps",
            warmup_steps=50,  # –ú–µ–Ω—å—à–µ warmup –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
            weight_decay=0.01,
            logging_dir=str(self.output_dir / "logs"),
            report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º wandb/tensorboard
            remove_unused_columns=False,
            dataloader_pin_memory=False,
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ!")
    
    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è...")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            raise
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        logger.info("–°–æ—Ö—Ä–∞–Ω—è—é –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
            self.model.save_pretrained(self.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer.save_pretrained(self.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è
            config = {
                "base_model": self.base_model,
                "previous_training": str(self.trained_model_path),
                "training_info": {
                    "method": "QLoRA_Continued",
                    "output_dir": str(self.output_dir),
                    "type": "continued_training"
                }
            }
            
            with open(self.output_dir / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument("--data", required=True, help="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--base-model", default="meta-llama/Llama-2-7b-hf", 
                       help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)")
    parser.add_argument("--trained-model", required=True,
                       help="–ü—É—Ç—å –∫ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--output-dir", default="trained_model_continued", 
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--learning-rate", type=float, default=1e-4, 
                       help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—Å–Ω–∏–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)")
    parser.add_argument("--epochs", type=int, default=2, 
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–º–µ–Ω—å—à–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)")
    parser.add_argument("--batch-size", type=int, default=4, 
                       help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--lora-r", type=int, default=16, 
                       help="–†–∞–Ω–≥ LoRA")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    if not Path(args.data).exists():
        print(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.data}")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    if not Path(args.trained_model).exists():
        print(f"‚ùå –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.trained_model}")
        return
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        trainer = ContinueTrainingTrainer(
            base_model=args.base_model,
            trained_model_path=args.trained_model,
            output_dir=args.output_dir
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        trainer.load_trained_model_and_tokenizer()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if not hasattr(trainer, 'lora_config'):
            trainer.setup_lora_config(r=args.lora_r)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        train_dataset = trainer.load_training_data(args.data)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.setup_training(
            train_dataset=train_dataset,
            learning_rate=args.learning_rate,
            num_epochs=args.epochs,
            batch_size=args.batch_size
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.train()
        
        print(f"\n‚úÖ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {args.output_dir}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
