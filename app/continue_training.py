#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
"""

import json
import os
import argparse
from pathlib import Path
from typing import Dict, List, Optional
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
    PeftModel
)
from datasets import Dataset
import logging
import psutil  # –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω—ã
from prompt_templates import PromptTemplates, create_enhanced_training_text

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –¥–ª—è –ª—É—á—à–µ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# –õ–∏–º–∏—Ç—ã –¥–ª—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏
os.environ['OMP_NUM_THREADS'] = '8'  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º OpenMP –ø–æ—Ç–æ–∫–∏
os.environ['MKL_NUM_THREADS'] = '8'  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º MKL –ø–æ—Ç–æ–∫–∏
os.environ['NUMEXPR_NUM_THREADS'] = '8'  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º NumExpr –ø–æ—Ç–æ–∫–∏

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAM –¥–ª—è PyTorch
import torch
torch.set_num_threads(8)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ PyTorch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('continue_training.log'),  # –õ–æ–≥–∏ –≤ —Ñ–∞–π–ª
        logging.StreamHandler()  # –õ–æ–≥–∏ –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)
logger = logging.getLogger(__name__)


def log_memory_usage():
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    memory = psutil.virtual_memory()
    logger.info(f"RAM: {memory.percent:.1f}% –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ({memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB)")
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / (1024**3)
        gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        logger.info(f"GPU: {gpu_memory:.1f}GB / {gpu_total:.1f}GB")


class ContinueTrainingTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, 
                 base_model: str = "meta-llama/Llama-2-7b-hf",
                 trained_model_path: str = "trained_model",
                 output_dir: str = "trained_model_continued",
                 device: str = "auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)
            trained_model_path: –ü—É—Ç—å –∫ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (auto/cpu/cuda)
        """
        self.base_model = base_model
        self.trained_model_path = Path(trained_model_path)
        self.output_dir = Path(output_dir)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU (CUDA)")
            else:
                self.device = "cpu"
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        else:
            self.device = device
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è")
        logger.info(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
        logger.info(f"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {trained_model_path}")
        logger.info(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
        log_memory_usage()  # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
    
    def load_trained_model_and_tokenizer(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        logger.info("–ó–∞–≥—Ä—É–∂–∞—é —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            if (self.trained_model_path / "tokenizer.json").exists():
                logger.info("–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.trained_model_path,
                    trust_remote_code=True,
                    padding_side="right"
                )
            else:
                # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                logger.info("–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model,
                    trust_remote_code=True,
                    padding_side="right"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            logger.info("–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            if self.device == "cpu":
                # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32 –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU
                base_model = base_model.to("cpu")
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                # –î–ª—è CPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º k-bit –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É
                logger.info("–ú–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ CPU")
            else:
                # –î–ª—è GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –∏ device_map
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model,
                    torch_dtype=torch.float16,
                    device_map=self.device,
                    trust_remote_code=True,
                    max_memory={0: "14GB"},  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
                    low_cpu_mem_usage=True  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ CPU
                )
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è
                logger.info("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –º–æ–¥–µ–ª—å –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è...")
                base_model = prepare_model_for_kbit_training(base_model)
                logger.info("–ú–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            logger.info("–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
            if (self.trained_model_path / "adapter_config.json").exists():
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å inference_mode=False –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
                    self.model = PeftModel.from_pretrained(
                        base_model, 
                        self.trained_model_path,
                        is_trainable=True  # –í–ê–ñ–ù–û: –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
                    )
                    logger.info("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: {e}")
                    logger.info("–ü—ã—Ç–∞—é—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ is_trainable...")
                    self.model = PeftModel.from_pretrained(base_model, self.trained_model_path)
                    logger.info("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –±–µ–∑ is_trainable")
            else:
                logger.warning("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–µ...")
                self.setup_lora_config()
                self.model = get_peft_model(base_model, self.lora_config)
            
            # –í–ê–ñ–ù–û: –í–∫–ª—é—á–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–∏
            logger.info("–í–∫–ª—é—á–∞—é —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è...")
            self.model.train()
            
            # –í–ê–ñ–ù–û: –í–∫–ª—é—á–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
            for name, param in self.model.named_parameters():
                if 'lora' in name.lower():
                    param.requires_grad = True
                    logger.debug(f"–í–∫–ª—é—á–µ–Ω –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {name}")
            
            # –û—Ç–∫–ª—é—á–∞–µ–º use_cache –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å gradient checkpointing
            if hasattr(self.model, 'config'):
                self.model.config.use_cache = False
                logger.info("–û—Ç–∫–ª—é—á–µ–Ω use_cache –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å gradient checkpointing")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é
            logger.info("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö:")
            self.model.print_trainable_parameters()
            
            logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            log_memory_usage()  # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def setup_lora_config(self, 
                          r: int = 16,
                          lora_alpha: int = 32,
                          lora_dropout: float = 0.1,
                          target_modules: Optional[List[str]] = None):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        
        Args:
            r: –†–∞–Ω–≥ LoRA
            lora_alpha: –ê–ª—å—Ñ–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä LoRA
            lora_dropout: Dropout –¥–ª—è LoRA
            target_modules: –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è LoRA
        """
        logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
        if target_modules is None:
            logger.info("–û–ø—Ä–µ–¥–µ–ª—è—é target_modules –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...")
            if "qwen" in self.base_model.lower():
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è Qwen –º–æ–¥–µ–ª–µ–π
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            elif "llama" in self.base_model.lower():
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            elif "gpt" in self.base_model.lower():
                target_modules = ["c_attn", "c_proj", "c_fc", "c_proj"]
            elif "bert" in self.base_model.lower():
                target_modules = ["query", "key", "value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
            else:
                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ target_modules: {target_modules}")
        
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            bias="none"
        )
        
        logger.info("LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")
    
    def load_training_data(self, data_path: str) -> Dataset:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            data_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑: {data_path}")
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
            if 'qa_pairs' in data:
                qa_pairs = data['qa_pairs']
            elif isinstance(data, list):
                qa_pairs = data
            else:
                raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º
            training_texts = []
            for qa in qa_pairs:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω
                text = create_enhanced_training_text(qa['question'], qa['answer'], "enhanced")
                training_texts.append(text)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –°–æ–∑–¥–∞–µ–º Dataset
            dataset = Dataset.from_dict({"text": training_texts})
            return dataset
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def tokenize_function(self, examples):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.debug(f"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –±–∞—Ç—á –∏–∑ {len(examples['text'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
        result = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors=None  # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã PyTorch –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )
        logger.debug(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä—ã: {len(result['input_ids'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return result
    
    def setup_training(self, 
                      train_dataset: Dataset,
                      learning_rate: float = 1e-4,  # –°–Ω–∏–∂–∞–µ–º LR –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
                      num_epochs: int = 2,          # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
                      batch_size: int = 4,
                      gradient_accumulation_steps: int = 4):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            train_dataset: –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—Å–Ω–∏–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–º–µ–Ω—å—à–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            gradient_accumulation_steps: –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è...")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        logger.info("–ù–∞—á–∏–Ω–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        try:
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π batch_size –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ç–∫–ª—é—á–∞–µ–º return_tensors
            tokenized_dataset = train_dataset.map(
                self.tokenize_function,
                batched=True,
                batch_size=10,  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
                remove_columns=train_dataset.column_names
            )
            logger.info(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(tokenized_dataset)}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
            raise
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
        logger.info("–°–æ–∑–¥–∞—é TrainingArguments...")
        try:
            training_args = TrainingArguments(
                output_dir=str(self.output_dir),
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=gradient_accumulation_steps,
                learning_rate=learning_rate,
                fp16=(self.device != "cpu"),  # –û—Ç–∫–ª—é—á–∞–µ–º fp16 –¥–ª—è CPU
                logging_steps=10,
                save_steps=100,
                eval_steps=100,
                save_strategy="steps",
                warmup_steps=50,  # –ú–µ–Ω—å—à–µ warmup –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
                weight_decay=0.01,
                logging_dir=str(self.output_dir / "logs"),
                report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º wandb/tensorboard
                remove_unused_columns=False,
                dataloader_pin_memory=(self.device != "cpu"),  # –û—Ç–∫–ª—é—á–∞–µ–º pin_memory –¥–ª—è CPU
                dataloader_num_workers=0,  # –û—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                gradient_checkpointing=False,  # –û—Ç–∫–ª—é—á–∞–µ–º gradient checkpointing –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
            )
            logger.info("TrainingArguments —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ TrainingArguments: {e}")
            raise
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ!")
    
    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è...")
        
        try:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            logger.info("–ü—Ä–æ–≤–µ—Ä—è—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º...")
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è
            self.model.train()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
            
            if trainable_params == 0:
                logger.warning("–ù–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤! –ü—ã—Ç–∞—é—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å...")
                
                # –ü—ã—Ç–∞–µ–º—Å—è –≤–∫–ª—é—á–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                for name, param in self.model.named_parameters():
                    if 'lora' in name.lower() or 'adapter' in name.lower():
                        param.requires_grad = True
                        logger.info(f"–í–∫–ª—é—á–µ–Ω –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è: {name}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                logger.info(f"–ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
                
                if trainable_params == 0:
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            raise
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        logger.info("–°–æ—Ö—Ä–∞–Ω—è—é –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
            self.model.save_pretrained(self.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer.save_pretrained(self.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è
            config = {
                "base_model": self.base_model,
                "previous_training": str(self.trained_model_path),
                "training_info": {
                    "method": "QLoRA_Continued",
                    "output_dir": str(self.output_dir),
                    "type": "continued_training"
                }
            }
            
            with open(self.output_dir / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument("--data", required=True, help="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--base-model", default="meta-llama/Llama-2-7b-hf", 
                       help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)")
    parser.add_argument("--trained-model", required=True,
                       help="–ü—É—Ç—å –∫ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--output-dir", default="trained_model_continued", 
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--learning-rate", type=float, default=1e-4, 
                       help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—Å–Ω–∏–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)")
    parser.add_argument("--epochs", type=int, default=2, 
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–º–µ–Ω—å—à–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)")
    parser.add_argument("--batch-size", type=int, default=4, 
                       help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--lora-r", type=int, default=16, 
                       help="–†–∞–Ω–≥ LoRA")
    parser.add_argument("--device", default="auto", 
                       choices=["auto", "cpu", "cuda"],
                       help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (auto/cpu/cuda)")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    if not Path(args.data).exists():
        print(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.data}")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    if not Path(args.trained_model).exists():
        print(f"‚ùå –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.trained_model}")
        return
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        trainer = ContinueTrainingTrainer(
            base_model=args.base_model,
            trained_model_path=args.trained_model,
            output_dir=args.output_dir,
            device=args.device
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        trainer.load_trained_model_and_tokenizer()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if not hasattr(trainer, 'lora_config'):
            trainer.setup_lora_config(r=args.lora_r)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        train_dataset = trainer.load_training_data(args.data)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.setup_training(
            train_dataset=train_dataset,
            learning_rate=args.learning_rate,
            num_epochs=args.epochs,
            batch_size=args.batch_size
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.train()
        
        print(f"\n‚úÖ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {args.output_dir}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
